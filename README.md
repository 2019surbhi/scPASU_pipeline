# scPASU pipeline

Alternative Polyadenylation (APA) analysis utilizes 3' end sequencing data. Interestingly, most of the scRNA-seq data is also generated by 3' end sequencing. Thus, while the data generated by 3' scRNA-seq methods is not suitable to study alternative splicing (AS), this data can be readily repurposed for APA analysis. Since APA is context-dependent, it necessitates the generation of a dataset-specific reference to capture poly(A) sites used in a given biological context. This is particularly important in case of novel poly(A) site usage which occurs, for example, in case of several cancers due to mutations or other regulatory mechanisms. We therefore built a novel pipeline called Single cell Poly(A) Site Usage (scPASU) to achieve this goal.

## Installation

Create a conda environment. It is recommended that you install this new, faster dependency [solver](https://www.anaconda.com/blog/a-faster-conda-for-a-growing-community) first.
```bash
conda create --name scPASU_env python=3.10
conda activate scPASU_env
```
Install required R libraries.
```bash
conda install -c conda-forge r-essentials r-base
```
```R
R_libs <- c('gtools','argparser','reshape','Seurat','pheatmap','factoextra','BiocManager','devtools')
install.packages(R_libs)
Bioconductor_R_libs <- c('GenomicRanges','rtracklayer','Biostrings','Rsubread','DEXSeq','apeglm','DESeq2','BSgenome.Hsapiens.UCSC.hg38','BSgenome.Mmusculus.UCSC.mm10')
BiocManager::install(Bioconductor_R_libs)
devtools::install_github("jeffbhasin/goldmine")
```
If you can't install any of the R libraries from R, try doing so with conda. For example:
```bash
conda install conda-forge::r-factoextra
conda install conda-forge::r-devtools
```
Install the following:
* [polyAfilter](https://github.com/MarekSvob/polyAfilter)
* [umi-tools](https://umi-tools.readthedocs.io/en/latest/INSTALL.html)
* [samtools](https://anaconda.org/bioconda/samtools)
* [subset-bam](https://github.com/10XGenomics/subset-bam)
* [bioperl](https://anaconda.org/bioconda/perl-bioperl)
    * If [perl](https://www.perl.org/get.html), make sure to install `strict`, `warnings`, `Getopt::Long`, `Pod::Usage` and `Bio::DB::Fasta`
* [picard](https://broadinstitute.github.io/picard/)
* [MACS2](https://pypi.org/project/MACS2/)
* [deeptools](https://deeptools.readthedocs.io/en/develop/content/installation.html)
* [bedtools](https://bedtools.readthedocs.io/en/latest/content/installation.html)
* [bowtie2](https://www.metagenomics.wiki/tools/bowtie2/install)
* [UCSC utilities](https://hgdownload.soe.ucsc.edu/admin/exe/linux.x86_64.v369/): `bigWigToBedGraph`,`bedToBigBed`,`bedGraphToBigWig`
* [bamtofastq](https://github.com/10XGenomics/bamtofastq/releases)
* [cellranger](https://www.10xgenomics.com/support/software/cell-ranger/latest/tutorials/cr-tutorial-in)

## Getting started: Populate variables

To make a copy of the template scripts in a new directory for your analysis, fill in variables in `scPASU_variable_initialize.sh` and

```bash
bash scPASU_variable_initialize.sh
```
**Note**: Variable auto-population applies to scripts from step 1–4, which need to be processed on a high-performance computing cluster (HPC) because of the computational resources they might require. And users may still need to change the arguments of each script, especially those shown in blocks of codes below, as they see fit for their data.

## Step 1: Data preprocessing

scPASU is built to dovetail from standard scRNA-seq analyses. It first processes the BAM files from cellranger outputs. 

Create a database from your reference gene annotation for the purpose of genomic-A reads filtering later.
```bash
bsub < 1a_prepare_gtf_db.lsf
``` 
Remove PCR duplicates and ambiguously mapped reads per cell (`1b_preprocess_bam_dedup_cleanup.lsf`) before two rounds of genomic-A reads filtering. The first round (`1b_preprocess_bam_genomicA_filtering_1stround.lsf`) searches for genomic A sequences including introns, while the second round (`1b_preprocess_bam_genomicA_filtering_2ndround.lsf`) does not include introns in the search. 
Genomic-A filtering arguments (default settings):
```bash
MISM=3 #number of mismatches allowed
NTHREADS=32 #number of threads for parallel computing
COVLEN=300 #maximal coverage distance 
MINSNRLEN=10 #minimum length of A-SNRs
```
If applied, processed BAM files need to be subsetted for the compartment/cell type of interest (`1c_subset_bam.lsf`). Beforehand, `mkdir -p ${outdir}/barcode/` and place files containing cell barcodes of the compartment/cell type of interest from each sample there. These files should contain only these barcodes, one per row, no quotation mark, and the barcodes should be the same as those found in the `barcodes.tsv.gz` output of `cellranger`.

Since these scripts need to be performed for each sample separately, users can automate submission of jobs for multiple samples with `job_submission_bysample.sh`. **Make sure to hash out all but one script at a time**.
```bash
bash job_submission_bysample.sh 
``` 

BAM files from individual samples after genomic-A filtering (`1d1_merge_bam.lsf`) and before genomic-A filtering (`1d2_merge_bam.lsf`) were merged and parsed by strand.
```bash
bsub < 1d1_merge_bam.lsf
bsub < 1d2_merge_bam.lsf 
``` 
## Step 2: Peak calling and retention

From step 2 to halfway through step 3, the pipeline will be run in two parallel prongs called **all_filtered_reads** (reads that have survived all filtering in step 1) and **polyA_reads** (poly(A) junction reads identified from all deduplicated and non-ambiguously mapped reads) to give the latter set of reads more weight. True poly(A) junction reads could be accidentally removed by ``polyAfilter.py``. The [samToPolyA.pl](https://github.com/julienlag/samToPolyA/blob/master/samToPolyA.pl) script, which we use to detect poly(A) junction reads, potentially allows distinguishing true poly(A) junctions from genomic A primed reads based on soft-clipping.

| all_filtered_reads | polyA_reads |
|----------|----------|
| Find clusters of reads with MACS2: <pre><code class="language-bash"># Arguments&#13;gsize=hs #Effective genome size. Change according to your organism &#13;read_extsize=200 #size to extend towards 5'->3' reads shorter than this value to </code></pre> <pre><code class="language-bash">bsub < 2a_all_filtered_reads_peak_call_summits.lsf </code></pre>| Find poly(A) junction reads: <pre><code class="language-bash"># Arguments&#13;minClipped=9 #minimum length of soft-clipped poly(A) tails &#13;minAcontent=0.85 #minimum length of A or T tail required to call a PolyA site </code></pre> <pre><code class="language-bash">bsub < 2a_polyAreads_sam_to_polyA.lsf </code></pre>|
| Find poly(A) junction reads: <pre><code class="language-bash"># Arguments&#13;minClipped=9 &#13;minAcontent=0.85 </code></pre> <pre><code class="language-bash">bsub < 2a_all_filtered_reads_sam_to_polyA.lsf </code></pre>| Subset BAM file by poly(A) junction reads:<pre><code class="language-bash">bsub < 2b1_polyAreads_filterbam_byreadnames.lsf</code></pre>Find clusters of poly(A) junction reads:<pre><code class="language-bash"># Arguments&#13;gsize=hs &#13;read_extsize=200 </code></pre><pre><code class="language-bash">bsub < 2b2_polyAreads_peak_call_summits.lsf</code></pre>|

Split multimodal peaks by regions of no alignments between their summits if there are any (`2c_split_peak_ref.lsf`). Before being split, for poly(A) site usage quantification later, peaks are extended by `peak_extsize`, which should be ≤ MACS2 detected tag size to prevent coordinate overlapping since by default this is maximum gap between significant sites to cluster them together. Two prongs can be run with one script from here on. 
```bash
# Argument
binsize=1 #Bin size to compute alignment signals, i.e. number of reads per bin
read_extsize=200 #size for bamCoverage to extend towards 5'->3' reads shorter than this value to before computing alignment signals. Should be the same as previous read_extsize to mimic MACS2 pre-processing of peaks.
peak_extsize=100 #size to extend peaks by. Peaks from the all_filtered reads prong will be extended 5'->3' while polyA_reads 3'->5'. 
ncores=16 #number of threads for parallel computing
```
Peaks are only retained if they are supported by poly(A) junction reads (`2d_peak_ref.lsf`). We still require the clusters of poly(A) junction reads to overlap with at least 1 poly(A) junction reads as spliced alignments can give rise to peaks not immediately adjacent to a poly(A) tail.
```bash
# Argument
all_filtered_reads_min_polya=3
polyA_reads_min_polya=1
```
```bash
bsub < 2d_peak_ref.lsf
```

## Step 3: Peak clean up and assignment

Add gene annotations to peaks. First we need to create an annotation reference from the GTF file used by cellranger. Transcripts of each gene are collapsed into distinct intervals called Transcriptional Units (TU). This is our new gene unit. The TU reference only needs to be created once for each GTF file.
```bash
bsub < create_turef.lsf 
```
TUs are initially extended by 5kb at the 3' end, and these flank regions will be updated at each peak filtering round to end at the most downstream peaks that overlap with these flanks. We first retain only peaks that overlap with one and only one TU or its flank.
```bash
bsub < 3a_assign_tu.lsf
``` 

Next, peaks with inferred processing region (PR) length > `maxwidth` bp are filtered out. We then implement a filtering step which retains peaks with at least one poly(A) hexamer signal (`PAS.fa`) within or certain number of bases upstream/downstream (`pr_extn`) from their PR regions and only rescues peaks less than `tes_prox_distance` bases away from an annotated transcription end site. This step is optional because cancer mutations can make the reference genome used for alignments not reliable for PAS hexamer search. For the rest of step 3, users can process peaks with `PAS_filtering` both `false` and `true` and look at how drastically PAS filtering affects peak counts, on top of biological knowledge, to decide if this filtering round is appropriate for their data.
Lastly, within each prong, per gene read percentage is calculated for each peak, and lowly used peaks (<`min_cov`%) are filtered out. 
```bash
# Argument
maxwidth=1000
PAS_filtering=false
pr_extn="5,50" # Direction comma-separated by distance.
tes_prox_distance=100
min_cov=10
cores=16 #number of threads for parallel computing
```
```bash
bsub < 3b_update_peak_ref.lsf
``` 

Peaks from two prongs are merged into a final peak reference. Redundancy arising from regions having overlapping between the two peak sets is resolved by prioritizing whichever set with more peaks to achieve a higher resolution of poly(A) sites. Ties are broken by choosing the set with a larger width average or the peaks from the `all_filtered_reads` prong.
```bash
bsub < 3c_merge_two_prongs.lsf
``` 

The merged peak reference is now annotated for genomic context. Peaks are classified into five categories, in order of priority to resolve classifications into multiple categories: 3' UTR, TSS-proximal, Exonic, Intronic and Flank. Proximity to a TSS, short for transcriptional start site, of a TU is defined to be less than `dist_pct` of the length of that TU, calculated with its last updated flank region. Here we also identify transcripts that both overlap with multiple exonic peaks and have multiple exons overlapping with those peaks. Out of these transcripts, only those with peak-overlapped exons adding up to `frag_length` bases maximum in length, which should be determined by the average fragment size in the library, along with the exonic peaks overlapping with them are inputs for the next peak annotation that identifies and flags fragmented peak, i.e. peaks arising out of spliced alignments.
```bash
# Argument
PAS_filtering=false
dist_pct=0.1
frag_length=500
ncores=16 #number of threads for parallel computing
```
```bash
bsub < 3d_peak_classification.lsf
```

A set of peaks will be flagged as one fragmented peak if each of them is made up of ≥ `spliced_read_pct_thres` spliced reads and these spliced reads when realigned against the right exome give rise to a peak made up of ≥ `realign_peak_read_pct_thres` of the total reads across the peaks. These thresholds can be adjusted based on distribution plots (`*realign_peak_read_pct_hist.png`, `*spliced_read_pct_hist.png`). The realign peaks are called with arguments `gap_threshold` and `min_coverage`, which respectively separate multiple cluster of reads on one exome and remove small clusters. 
```bash
# Argument
PAS_filtering=false
binsize=1
gap_threshold=0
min_coverage=10
spliced_read_pct_thres=40
realign_peak_read_pct_thres=40
ncores=32 #number of threads for parallel computing
```
```bash
bsub < 3e_identify_fragmented_peaks.lsf
```

### Poly(A) site reference quality check

Obtain poly(A) site reference statistics. The most time-consuming part here is to obtain read statistics from BAM files. You can hash out lines start with `samtools stats` if you need to run this script multiple times.
```bash
bsub < pa_ref_stats.lsf
``` 

Compute nucleotide frequency around PRs of peaks in the reference obtained. If `kmer_to_plot=all`, frequency of each nucleotide at each position will be an interquartile range. 
```bash
# Argument
PAS_filtering=false
kmer_to_plot=first #Compute nucleotide frequency around the starts of PRs (first), or ends (last), or all nucleotides (all).
kmer_size=201 #Coverage distance on each side x 2 + 1
ncore=16 #number of threads for parallel computing
```
```bash
bsub < nuc_freq_plot.lsf
```

See `hexamer_freq_plot.R` for hexamer frequency counting plot and `PA_peak_ref_stats.R` for metagene plot, amongst other plots.

## Step 4: Poly(A) count-by-cell matrix generation

Create a cellranger reference with the poly(A) site reference obtained.
```bash
bsub < cellranger_mkref_refgenome-scPASUref.lsf
```
The processed BAM files of each sample are converted back to fastq files (`4a_BamToFastq.lsf`) and then fed into cellranger along with the poly(A) site reference to produce peak count matrices by cell matrix for downstream analyses (`4b_cellranger-peakcount.lsf`). Gene count matrices of genomic-A filtered reads can also be generated (`4b_cellranger-genecount.lsf`). Again use `job_submission_bysample.sh` to automate submission of jobs for multiple samples.
```bash
bash job_submission_bysample.sh 
```

## Step 5: Alternative poly(A) site usage testing

Users should merge peak/gene count matrices from individual samples and fix the cell barcodes to match previous scRNA-seq analyses (`5a_merge_per_sample_counts.R`). See `5b1_APA_testing.R` for APA testing, `5b2_DEG_testing.R` for pseudo-bulk DEG testing, and `5c_multimodal_clustering.R` for integrating poly(A) site usage with gene expression to improve cell clustering.
